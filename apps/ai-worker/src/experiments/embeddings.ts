/**
 * ============================================
 * EXPERIMENT 1: COMPRENDRE LES EMBEDDINGS
 * ============================================
 *
 * Un embedding est une repr√©sentation num√©rique d'un texte.
 * C'est un vecteur (tableau de nombres) qui capture le "sens" du texte.
 *
 * Pourquoi c'est puissant ?
 * - Deux textes similaires auront des vecteurs proches
 * - On peut mesurer la "distance" entre deux textes
 * - Permet la recherche par sens, pas juste par mots-cl√©s
 *
 * Dans ce script, tu vas :
 * 1. G√©n√©rer des embeddings pour diff√©rentes phrases
 * 2. Calculer la similarit√© entre elles
 * 3. Visualiser comment le mod√®le comprend le sens
 */

import OpenAI from 'openai';

// ============================================
// CONFIGURATION
// ============================================

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

// Le mod√®le d'embedding qu'on utilise
// text-embedding-3-small : 1536 dimensions, bon rapport qualit√©/prix
// text-embedding-3-large : 3072 dimensions, plus pr√©cis mais plus cher
const EMBEDDING_MODEL = 'text-embedding-3-small';

// ============================================
// FONCTION PRINCIPALE : G√©n√©rer un embedding
// ============================================

/**
 * G√©n√®re un embedding pour un texte donn√©
 *
 * @param text - Le texte √† transformer en vecteur
 * @returns Un tableau de nombres (le vecteur)
 */
async function getEmbedding(text: string): Promise<number[]> {
  const response = await openai.embeddings.create({
    model: EMBEDDING_MODEL,
    input: text,
  });

  // L'API retourne un tableau d'embeddings (un par input)
  // On prend le premier car on n'a envoy√© qu'un seul texte
  return response.data[0].embedding;
}

/**
 * G√©n√®re des embeddings pour plusieurs textes en une seule requ√™te
 * (Plus efficace que d'appeler getEmbedding() plusieurs fois)
 */
async function getEmbeddings(texts: string[]): Promise<number[][]> {
  const response = await openai.embeddings.create({
    model: EMBEDDING_MODEL,
    input: texts,
  });

  return response.data.map((item) => item.embedding);
}

// ============================================
// SIMILARIT√â COSINUS
// ============================================

/**
 * Calcule la similarit√© cosinus entre deux vecteurs
 *
 * La similarit√© cosinus mesure l'angle entre deux vecteurs :
 * - 1.0 = identiques (angle de 0¬∞)
 * - 0.0 = orthogonaux (angle de 90¬∞, pas de relation)
 * - -1.0 = oppos√©s (angle de 180¬∞)
 *
 * Pour les embeddings de texte, on obtient g√©n√©ralement :
 * - > 0.8 : tr√®s similaires
 * - 0.6-0.8 : li√©s
 * - < 0.5 : peu de relation
 */
function cosineSimilarity(vecA: number[], vecB: number[]): number {
  if (vecA.length !== vecB.length) {
    throw new Error('Vectors must have the same length');
  }

  let dotProduct = 0;
  let normA = 0;
  let normB = 0;

  for (let i = 0; i < vecA.length; i++) {
    dotProduct += vecA[i] * vecB[i];
    normA += vecA[i] * vecA[i];
    normB += vecB[i] * vecB[i];
  }

  return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));
}

// ============================================
// EXP√âRIENCES
// ============================================

async function runExperiments() {
  console.log('üß™ EXP√âRIENCE 1: Comprendre les Embeddings\n');
  console.log('='.repeat(60));

  // ----------------------------------------
  // Test 1: Phrases similaires vs diff√©rentes
  // ----------------------------------------
  console.log('\nüìä TEST 1: Similarit√© s√©mantique\n');

  const sentences = [
    'Le chat dort sur le canap√©',
    'Le f√©lin sommeille sur le sofa', // Synonymes ‚Üí devrait √™tre tr√®s similaire
    'Python est un langage de programmation', // Sujet diff√©rent ‚Üí devrait √™tre √©loign√©
    'Le chien joue dans le jardin', // M√™me structure, sujet proche ‚Üí moyennement similaire
  ];

  console.log('Phrases √† comparer:');
  sentences.forEach((s, i) => console.log(`  ${i + 1}. "${s}"`));
  console.log();

  // G√©n√©rer tous les embeddings en une requ√™te
  const embeddings = await getEmbeddings(sentences);

  console.log(`‚úÖ Embeddings g√©n√©r√©s (${embeddings[0].length} dimensions chacun)\n`);

  // Calculer la matrice de similarit√©
  console.log('Matrice de similarit√©:');
  console.log('(1.00 = identique, 0.00 = aucune relation)\n');

  // En-t√™te
  console.log('        ' + sentences.map((_, i) => `  [${i + 1}]  `).join(''));

  for (let i = 0; i < sentences.length; i++) {
    let row = `  [${i + 1}]   `;
    for (let j = 0; j < sentences.length; j++) {
      const sim = cosineSimilarity(embeddings[i], embeddings[j]);
      row += ` ${sim.toFixed(2)}  `;
    }
    console.log(row);
  }

  // ----------------------------------------
  // Test 2: Visualiser les dimensions
  // ----------------------------------------
  console.log('\nüìä TEST 2: Visualiser un embedding\n');

  const sampleEmbedding = embeddings[0];
  console.log(`Embedding de "${sentences[0]}":`);
  console.log(`  - Nombre de dimensions: ${sampleEmbedding.length}`);
  console.log(`  - Premi√®res valeurs: [${sampleEmbedding.slice(0, 5).map((v) => v.toFixed(4)).join(', ')}, ...]`);
  console.log(`  - Min: ${Math.min(...sampleEmbedding).toFixed(4)}`);
  console.log(`  - Max: ${Math.max(...sampleEmbedding).toFixed(4)}`);
  console.log(`  - Moyenne: ${(sampleEmbedding.reduce((a, b) => a + b, 0) / sampleEmbedding.length).toFixed(4)}`);

  // ----------------------------------------
  // Test 3: Recherche par similarit√©
  // ----------------------------------------
  console.log('\nüìä TEST 3: Recherche par similarit√©\n');

  const query = 'Mon animal de compagnie se repose';
  console.log(`Requ√™te: "${query}"\n`);

  const queryEmbedding = await getEmbedding(query);

  const similarities = sentences.map((sentence, i) => ({
    sentence,
    similarity: cosineSimilarity(queryEmbedding, embeddings[i]),
  }));

  // Trier par similarit√© d√©croissante
  similarities.sort((a, b) => b.similarity - a.similarity);

  console.log('R√©sultats (tri√©s par pertinence):');
  similarities.forEach((item, i) => {
    const bar = '‚ñà'.repeat(Math.round(item.similarity * 20));
    console.log(`  ${i + 1}. [${item.similarity.toFixed(3)}] ${bar}`);
    console.log(`     "${item.sentence}"`);
  });

  // ----------------------------------------
  // Conclusion
  // ----------------------------------------
  console.log('\n' + '='.repeat(60));
  console.log('üìù CE QUE TU AS APPRIS:\n');
  console.log('1. Un embedding transforme du texte en vecteur de nombres');
  console.log('2. La similarit√© cosinus mesure la proximit√© s√©mantique');
  console.log('3. Des phrases synonymes ont des embeddings proches');
  console.log('4. On peut faire de la recherche s√©mantique avec ces vecteurs');
  console.log('\n‚û°Ô∏è  Prochaine √©tape: Stocker ces vecteurs dans Qdrant');
  console.log('   Ex√©cute: pnpm experiment:qdrant');
}

// ============================================
// POINT D'ENTR√âE
// ============================================

runExperiments().catch((error) => {
  console.error('‚ùå Erreur:', error.message);
  if (error.message.includes('API key')) {
    console.log('\nüí° As-tu configur√© OPENAI_API_KEY dans ton .env ?');
    console.log('   Copie .env.example vers .env et ajoute ta cl√© API');
  }
  process.exit(1);
});
